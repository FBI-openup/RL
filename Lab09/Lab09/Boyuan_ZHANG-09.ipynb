{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c1b9f6e",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "# CSC_52081_EP - Lab 09 - ADVERSARIAL BANDITS\n",
    "\n",
    "NOTE: The beginning of this lab is exactly the same as lab 3. The task for lab 9 is only the EXP implementation at the end of the lab. Still, it uses things that you have implemented in lab 3 for evaluation / comparison so you should first copy-paste your solutions of lab 3. \n",
    "\n",
    "### Main Objectives \n",
    "\n",
    "Today we will study bandits in the context of sequential decision making, and the exploration-exploitation tradeoff. You may find the required background in the lecture slides, lecture notes, and the references provided within. \n",
    " \n",
    "### Instructions\n",
    "\n",
    "Work your way through the notebook, and provide code where indicated by `# TODO` to complete the tasks. Check Moodle for details on how to submit your work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291048ed",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Imports\n",
    "\n",
    "First, we're going to import `numpy` and some utility functions/classes that we will use. Make sure the `bandit_machines.py` is in your working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe773891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773cc6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7aa1c3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from bandit_machines import BernoulliMAB, GaussianMAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd54d4bf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def randmax(a):\n",
    "    \"\"\" return a random maximum \"\"\"\n",
    "    a = np.array(a)  \n",
    "    max_indices = np.flatnonzero(a == a.max())\n",
    "    return np.random.choice(max_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6301d28",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Creating a bandit environment. \n",
    "\n",
    "What do we mean by a bandit environment or bandit machine? It's a kind of state-less enviornment. When generates from a particular generation, depending on which arm (or, action) is pulled (or, taken). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c6233",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arms = 2\n",
    "env_B = BernoulliMAB(n_arms=2,labels=[\"Arm 1\",\"Arm 2\"],means=[0.5,0.7])\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d48144",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### Evaluating a bandit learner\n",
    "\n",
    "The function given below executes one bandit algorithm on one multi-armed-bandit (MAB) instance. We will use this to compare bandit algorithms. Note that we compare them on a basis of **cumulative regret**. In this lab we are mainly interested in an empirical analysis of bandits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e8d906",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_run(env, pi, T=1000):\n",
    "    '''\n",
    "    Run a bandit agent on a bandit instance (environment) for T steps.\n",
    "    '''\n",
    "    r_log = []\n",
    "    a_log = []\n",
    "    pi.clear()\n",
    "    for t in range(T):\n",
    "        a = pi.act()\n",
    "        r = env.rwd(a)\n",
    "        pi.update(a,r)\n",
    "        r_log.append(r)\n",
    "        a_log.append(a)\n",
    "    return a_log, r_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c313e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_regret(env, selections):\n",
    "    \"\"\"Compute the pseudo-regret associated with a sequence of arm selections\"\"\"\n",
    "    best = np.max(env.means)*np.ones(len(selections))\n",
    "    real = np.array(env.means)[selections]\n",
    "    return np.cumsum(best - real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8704bc99",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### Baselines\n",
    "\n",
    "In the following, we implemented two naive bandit strategies: the greedy strategy (or Follow-the-Leader, `FTL`) and a strategy that explores arms uniformly at random (`UniformExploration`). \n",
    "\n",
    "Take note of the `class` structure (methods implemented), you will need to use the same structure in your implementations shortly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e50afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FTL:\n",
    "\n",
    "    \"\"\"Follow the Leader (a.k.a. greedy strategy)\"\"\"\n",
    "\n",
    "    def __init__(self,n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.nbDraws = np.zeros(self.n_arms)\n",
    "        self.cumRewards = np.zeros(self.n_arms)\n",
    "    \n",
    "    def act(self):\n",
    "        if (min(self.nbDraws)==0):\n",
    "            return randmax(-self.nbDraws)\n",
    "        else:\n",
    "            return randmax(self.cumRewards/self.nbDraws)\n",
    "\n",
    "    def update(self,a,r):\n",
    "        self.cumRewards[a] = self.cumRewards[a] + r\n",
    "        self.nbDraws[a] = self.nbDraws[a] + 1\n",
    "\n",
    "    def name(self):\n",
    "        return \"FTL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba03ca7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class UniformExploration:\n",
    "\n",
    "    \"\"\"a strategy that uniformly explores arms\"\"\"\n",
    "\n",
    "    def __init__(self,n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.nbDraws = np.zeros(self.n_arms)\n",
    "        self.cumRewards = np.zeros(self.n_arms)\n",
    "    \n",
    "    def act(self):\n",
    "        return np.random.randint(0,self.n_arms)\n",
    "\n",
    "    def update(self,arm,reward):\n",
    "        self.cumRewards[arm] = self.cumRewards[arm]+reward\n",
    "        self.nbDraws[arm] = self.nbDraws[arm] +1\n",
    "\n",
    "    def name(self):\n",
    "        return \"Uniform\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e915cfa2",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Below we run `FTL` on the simple Bernoulli bandit instance defined above, and we visualize its behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed6c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftl = FTL(n_arms)\n",
    "T = 200\n",
    "actions, rewards = evaluate_run(env_B, ftl, T)\n",
    "regret1 = cumulative_regret(env_B, actions)\n",
    "# Histogram of the number of arms selections\n",
    "plt.clf()\n",
    "plt.xlabel(\"Arms\", fontsize=14)\n",
    "plt.xticks(range(ftl.n_arms))\n",
    "plt.ylabel(\"Number of arms selections\", fontsize=14)\n",
    "plt.hist(actions, np.max(actions) + 1)\n",
    "plt.title(\"Number of selections of each arm\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f388ba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative regret as a function of time\n",
    "plt.figure(2)\n",
    "plt.clf()\n",
    "plt.xlabel(\"$t$\", fontsize=14)\n",
    "plt.ylabel(\"Cumulative regret\", fontsize=14)\n",
    "plt.title(\"Regret as a function of time\")\n",
    "plt.plot(range(T), regret1, 'black', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8865fec",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Let's compare regret between FTL and the Uniform algorithm.\n",
    "\n",
    "You can run the code more than once, and you should get a different result each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616c4a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare their regret\n",
    "ufm = UniformExploration(n_arms)\n",
    "T = 200\n",
    "actions, rewards = evaluate_run(env_B, ufm, T)\n",
    "regret2 = cumulative_regret(env_B, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e8b6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Time steps\", fontsize=14)\n",
    "plt.ylabel(\"Cumulative regret\", fontsize=14)\n",
    "plt.title(\"Regret as a function of time\")\n",
    "plt.plot(range(0, T), regret1,label=ftl.name())\n",
    "plt.plot(range(0, T), regret2,label=ufm.name())\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bbacd0",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### Comparison averaged over multiple runs\n",
    "\n",
    "Since the regret is defined as an **expectation**, we need several runs to estimate its value. We can also take a look at the distribution of the pseudo-regret. The function below gathers results accross multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd3777e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_runs(env, pi, T=1000, N=10):\n",
    "    '''\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        env : \n",
    "            bandit machine\n",
    "        pi : \n",
    "            bandit algorithm\n",
    "        N : int\n",
    "            number of experiments\n",
    "        T : int\n",
    "            number of trails per experiment\n",
    "    '''\n",
    "    R_log = np.zeros((N,T))\n",
    "    A_log = np.zeros((N,T),dtype=int)\n",
    "    for n in range(N):\n",
    "        np.random.seed()\n",
    "        A_log[n], R_log[n] = evaluate_run(env, pi, T)\n",
    "\n",
    "    return A_log, R_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0132e28",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "The following function will be useful for comparing arbitrary combinations of bandit algorithms empirically under a number (`N`) of runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63abf4d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_results(env, algorithms, T=1000, N=10, fname=None): \n",
    "\n",
    "    plt.clf()\n",
    "    plt.title(\"Cumulative Regret over %d Runs\" % N)\n",
    "\n",
    "    colours = ['b', 'r', 'g', 'm']\n",
    "\n",
    "    for j, pi in enumerate(algorithms):\n",
    "\n",
    "        A_log, R_log = evaluate_runs(env, pi, T, N)\n",
    "        Regret = np.array([cumulative_regret(env,A_log[n,:]) for n in range(N)])\n",
    "        meanRegret = np.mean(Regret, 0)\n",
    "        upperQuantile = np.quantile(Regret, 0.95, 0) \n",
    "        lowerQuantile = np.quantile(Regret, 0.05, 0)\n",
    "\n",
    "        plt.plot(range(T), meanRegret, linewidth=3.0, color=colours[j], label=\"\"+pi.name())\n",
    "        plt.plot(range(T), upperQuantile, linestyle=\"dashed\", color=colours[j])\n",
    "        plt.plot(range(T), lowerQuantile, linestyle=\"dashed\", color=colours[j])\n",
    "\n",
    "    plt.xlabel(\"$t$\", fontsize=10)\n",
    "    plt.ylabel(\"Cumulative regret\", fontsize=10)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    if fname is not None:\n",
    "        plt.savefig(fname,bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65716bed",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "So now let's compare FTL and Uniform in terms of expectation. What do you observe? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c93d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(env_B, [ftl, ufm])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c908dc0",
   "metadata": {
    "cell_marker": "r\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### Task \n",
    "\n",
    "Implement Epsilon Greedy ($\\epsilon$) in the `EpsilonGreedy` class below, with the same structure (method definitions) as `FTL` and `Uniform` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19f5f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "\n",
    "    \"\"\" Epsilon Greedy \"\"\"\n",
    "\n",
    "    def __init__(self, n_arms, epsilon=0.05):\n",
    "        self.n_arms = n_arms\n",
    "        self.epsilon = epsilon\n",
    "        self.clear()\n",
    "        # TODO  \n",
    "    def name(self):\n",
    "        return \"e-Greedy(%3.2f)\" % self.epsilon\n",
    "    def clear(self):\n",
    "        self.nbDraws = np.zeros(self.n_arms)\n",
    "        self.cumRewards = np.zeros(self.n_arms)\n",
    "    \n",
    "    def act(self):\n",
    "        if (np.random.rand() < self.epsilon):\n",
    "            return np.random.randint(0,self.n_arms)\n",
    "        else:\n",
    "            if (min(self.nbDraws)==0):\n",
    "                return randmax(-self.nbDraws)\n",
    "            else:\n",
    "                return randmax(self.cumRewards/self.nbDraws)\n",
    "    \n",
    "    def update(self,arm,reward):\n",
    "        self.cumRewards[arm] = self.cumRewards[arm]+reward\n",
    "        self.nbDraws[arm] = self.nbDraws[arm] +1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de45667b",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "Have a look at the performance of this method for different values of $\\epsilon$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fc0e57",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plot_results(env_B, [EpsilonGreedy(env_B.n_arms,0),EpsilonGreedy(env_B.n_arms,0.1),EpsilonGreedy(env_B.n_arms,0.01)], T=100, N=100)\n",
    "# \n",
    "\n",
    "# And with another bandit instance (Gaussian arms, this time)\n",
    "env_G = GaussianMAB(2)\n",
    "plot_results(env_G, [EpsilonGreedy(env_G.n_arms,0),EpsilonGreedy(env_G.n_arms,0.1),EpsilonGreedy(env_G.n_arms,0.01)], T=100, N=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70e8e3d",
   "metadata": {
    "cell_marker": "r\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### Task \n",
    "\n",
    "Implement UCB($\\alpha$) in the `UCB` class below, with the same structure (method definitions) as `FTL` and `Uniform` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60186b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB():\n",
    "\n",
    "\n",
    "    def __init__(self,n_arms,alpha=1/2):\n",
    "        self.n_arms = n_arms\n",
    "        self.alpha = alpha\n",
    "        self.clear()\n",
    "        # TODO \n",
    "\n",
    "    def name(self):\n",
    "        return \"UCB(%3.2f)\" % self.alpha\n",
    "    def clear(self):\n",
    "        self.nbDraws = np.zeros(self.n_arms)\n",
    "        self.cumRewards = np.zeros(self.n_arms)\n",
    "        self.ucbs = np.full(self.n_arms,np.inf)\n",
    "        self.time_step = 0\n",
    "    \n",
    "    def act(self):\n",
    "        self.time_step = self.time_step + 1\n",
    "        return randmax(self.ucbs)\n",
    "    \n",
    "    def update(self,arm,reward):\n",
    "        self.cumRewards[arm] = self.cumRewards[arm]+reward\n",
    "        self.nbDraws[arm] = self.nbDraws[arm] +1\n",
    "        for a in range(self.n_arms):\n",
    "            if self.nbDraws[a] > 0:  \n",
    "                self.ucbs[a] = self.cumRewards[a] / self.nbDraws[a] + np.sqrt(self.alpha * np.log(self.time_step) / self.nbDraws[a])\n",
    "\n",
    "    def render(self, env_G = None, fname=None):\n",
    "        arms = np.arange(self.n_arms)\n",
    "        avg_rewards = np.divide(self.cumRewards, self.nbDraws, where=self.nbDraws > 0, out=np.zeros_like(self.cumRewards))\n",
    "        confidence_bounds = self.ucbs - avg_rewards  \n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.bar(arms, avg_rewards, yerr=confidence_bounds, capsize=5, label=\"Estimated Value ± UCB\")\n",
    "        plt.xlabel(\"Arm\")\n",
    "        plt.ylabel(\"Estimated Reward\")\n",
    "        plt.title(f\"UCB Estimates (t={self.time_step})\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        if fname:\n",
    "            plt.savefig(fname)\n",
    "        else:\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97523f5",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "We'll just check that your UCB implementation is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5764fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ucb = UCB(env_G.n_arms)\n",
    "plot_results(env_G, [ucb], T=100, N=10)\n",
    "ucb.render(env_G) # ,fname=\"bandit_ucb_trained.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad38b686",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Let's try a different bandit machine. Namely, we'll try a mixed multi-arm bandit, with arms of different reward distributions, but you could experiment with others also. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e1ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bandit_machines import MixedMAB, TruncatedExponential, Bernoulli\n",
    "# You should play around with different types of bandit environments here \n",
    "env_M = MixedMAB([TruncatedExponential(2, 1), Bernoulli(0.3), TruncatedExponential(3.5, 1)])\n",
    "plot_results(env_M, [ucb], T=100, N=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c96266c",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "### Task \n",
    "\n",
    "Implement Thompson Sampling in the class provided below, with the same structure as the methods you implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa81d9cd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from scipy.stats import beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e96fb8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ThompsonSampling():\n",
    "\n",
    "    def __init__(self,n_arms):\n",
    "        ''' Bernoulli bandit machine (n_arms arms, each giving 0 or 1 with some probability) '''\n",
    "        self.n_arms = n_arms\n",
    "        # TODO  \n",
    "    def name(self):\n",
    "        return \"Thompson\"\n",
    "    \n",
    "    def clear(self):\n",
    "        self.alpha = np.ones(self.n_arms)  \n",
    "        self.beta = np.ones(self.n_arms)  \n",
    "\n",
    "    def act(self):\n",
    "        samples = []\n",
    "        for a in range(self.n_arms):\n",
    "            samples.append(beta.rvs(self.alpha[a], self.beta[a]))\n",
    "        return randmax(samples) \n",
    "\n",
    "    def update(self, a, r):\n",
    "        self.alpha[a] += r  \n",
    "        self.beta[a] += (1 - r)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e59fc79",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Finally, let's compare the three algorithms that you have implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe95035",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# \n",
    "#\n",
    "# Here, we carry out the comparison\n",
    "plot_results(env_B, [UCB(env_B.n_arms), EpsilonGreedy(env_B.n_arms), ThompsonSampling(env_B.n_arms)],N=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105f7252",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "### Something to Think About \n",
    "\n",
    "Regarding the Environment we used for Lab1 and Lab2, we could also use bandits in that scenario, namely, to if we had to decide on the best action to take *without* any observation. When considered as a 'rollout', this takes us closer and closer to Monte Carlo Tree Search and other algorithms in the domain of reinforcement learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf67c8cc",
   "metadata": {},
   "source": [
    "# LAB 9 : EXP3\n",
    "\n",
    "We finally explore the EXP3 algorithm. The EXP3 algorithm was proposed for adversarial bandits, but we will test it here using the stochastic bandits that we have seen above. \n",
    "\n",
    "EXP3 was originally proposed by [Auer et al., 1995](https://www.computer.org/csdl/proceedings-article/focs/1995/71830322/12OmNAYGlx6) and subsequently much used, refined and analyzed. An accessible presentation can be found in Chapter 11 of [Lattimore and Szepesvari's book](https://tor-lattimore.com/downloads/book/book.pdf). \n",
    "\n",
    "The idea of EXP3 is to use exponential weights based on an importance sampling estimate of the reward of each arm. Specifically, the algorithm does the following steps: \n",
    "- Initialize $\\hat{S}_{0,i} = 0$ for all arms $i$\n",
    "- For t = 1...n, \n",
    "    - calculate the distribution $P$ as $$P_{ti} = \\frac{\\exp(\\eta \\hat{S}_{t-1,i})}{\\sum_i \\exp(\\eta \\hat{S}_{t-1,i})}$$\n",
    "    - sample the arm $A_t$ at time $t$ according to this distribution\n",
    "    - upon receiving reward $X_t$ after drawing arm $A_t$, update the estimates of every arm using the loss-based importance-weighted estimator: $$\\hat{S}_{t,i} = \\hat{S}_{t-1,i} + 1 - \\frac{1_{A_t = i}\\cdot(1 - X_t)}{P_{ti}}$$\n",
    "    \n",
    "    \n",
    "Here $\\eta$ is a parameter called the learning rate. We assume that it is constant (i.e., does not depend on $t$). Note that we also assumed that the reward is in $[0, 1]$.\n",
    "\n",
    "### Task: Implement EXP3 with $\\eta$ as a parameter in the class below. \n",
    "\n",
    "Make sure to use the `eta` parameter as supplied to the function `__init__`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118fc690",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EXP3():\n",
    "    \"\"\"EXP3 algorithm with parameter eta\"\"\"\n",
    "\n",
    "    def __init__(self, n_arms, eta=1):\n",
    "        self.n_arms = n_arms\n",
    "        # TODO  \n",
    "        self.eta = eta\n",
    "        self.clear()\n",
    "\n",
    "    def name(self):\n",
    "        return \"Thompson\"\n",
    "    \n",
    "\n",
    "    def clear(self):\n",
    "        self.estimator = np.zeros(self.n_arms)\n",
    "        self.time_step = 0\n",
    "\n",
    "    def act(self):\n",
    "        self.time_step += 1\n",
    "        epsilon = 1e-10\n",
    "        self.distribution = np.exp(self.eta * self.estimator) / np.sum(np.exp(self.eta*self.estimator)) + epsilon\n",
    "        self.distribution /= np.sum(self.distribution)\n",
    "        return np.random.choice(self.n_arms,p=self.distribution)\n",
    "    \n",
    "    def update(self,arm,reward):\n",
    "        for i in range(self.n_arms):\n",
    "            self.estimator[i] += 1 - (i==arm)*(1-reward)/self.distribution[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125fe814",
   "metadata": {},
   "source": [
    "### Task: compare the results with the previously studied algorithms from lab 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03773cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(env_M, [UCB(env_M.n_arms), EpsilonGreedy(env_M.n_arms), ThompsonSampling(env_M.n_arms), EXP3(env_M.n_arms, eta = 0.5)],N=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eb7bc2",
   "metadata": {},
   "source": [
    "## Quiz.\n",
    "\n",
    "There is a pdf sheet associated with the lab with some quiz questions relating to bandits, as covered in the lecture. Replace the empty lists in the `answers` dictionary, such that answers[1] is a list of elements of 'a', 'b', 'c', 'd', 'e', 'f' (and so on, for the other questions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00128212",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {\n",
    "    1 : ['b','e'],\n",
    "    2 : ['a','b','c'], \n",
    "    3 : ['a','c'], \n",
    "    4 : ['b','c','e'], \n",
    "    5 : ['a','b','c','d'], \n",
    "    6 : ['a','b','d','e'],\n",
    "    7 : ['b','c','e','f']\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
