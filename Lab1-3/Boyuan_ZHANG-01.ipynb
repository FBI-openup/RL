{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ad68d0f",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "# CSC_52081_EP - Lab 01\n",
    "\n",
    "### Main Objectives \n",
    "\n",
    "Intelligent decision making involves several components. Today we study, in the context of a toy (low-dimensional, synthetic) example: *perception* (observation), *knowledge* (representation), *reasoning* (inference), and *acting* (decision-making). We will _not_ look at (today): learning and sequential decision making. Using probabalistic tools covered in the lecture (Bayesian networks, marginalization, ...), the objective is to design a rational/intelligent agent, i.e., an agent that maximizes its expected reward. \n",
    "\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Work your way through the notebook, and provide code where indicated to complete the tasks. Check Moodle for details on how to submit your work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b0519f",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Imports\n",
    "\n",
    "First, we're going to import `numpy` and some utility functions/classes that we will use. make sure the `environment.py` is in your working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f635082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0777d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8e740e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "And we'll use the environment defined in the file `environment.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fa4fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are using Google Colab, uncomment the following line (then comment again when you have run it): \n",
    "!wget https://www.lix.polytechnique.fr/~jread/courses/inf581/labs/01/environment.py\n",
    "from environment import Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d71baf6",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "\n",
    "## The Environment and the Agent\n",
    "\n",
    "Consider an intelligent agent as a robot tasked with catching rats in a storage room. It is night time, and the room is dark. You have to rely on auditory information only, but luckily the room is a mess with paper and other debris that means there are distinct sounds which are emitted by a rat as it touches different objects (namely, crinkling and rustling sounds). The room is rectangular, divided up into $n$ square tiles.  A rat has just entered the room (current time $t=1$). The agent waits $T$ seconds (i.e., until $t=T$), then makes a decision on if and where to pounce (in order to catch the rat)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba26116b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Let's instantiate an environment, and render a visualisation of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de784998",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "G = np.array([[1,3,0,2,4,1],\n",
    "              [2,1,0,3,0,3],\n",
    "              [4,0,3,0,2,0],\n",
    "              [3,1,2,3,0,4],\n",
    "              [2,0,0,0,1,1]])\n",
    "\n",
    "env = Environment(G)\n",
    "# Plot the environment in state 's_t = 4'\n",
    "fig, ax = env.render([4], None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeb7209",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "#### Notation\n",
    "\n",
    "Let $s_t \\in \\{1,\\ldots,n\\}$ denote the position (state) of the rat at time step $t$ (one of $n$ grid tiles); starting at some $s_1$ (entry tile). And $\\mathbf{x}_t \\in \\{0,1\\}^2$ is the 2-dimensional auditory observation at time $t$ (e.g., $\\mathbf{x}_t = [1,0]$ if there is a crinkle but no rustle, etc). The agent accumulates a sequence of **observations** $\\mathbf{x}_{1:T} = \\mathbf{x}_1,\\ldots,\\mathbf{x}_T$, with which to make the decision of taking **action** $a$ to pounce (denoting the tile upon which it pounces). The agent obtains **reward** $r(s,a) = 1_{s = a}$, i.e., catching the rat provides reward $1$ and $0$ otherwise. \n",
    "\n",
    "As an influence diagram, the problem described can be depicted as follows: \n",
    "\n",
    "![pgm](https://www.lix.polytechnique.fr/~jread/courses/inf581/labs/01/fig/pgm.png)\n",
    "\n",
    "Your task is to model this problem in Python code. Finally, the goal is to implement an `Agent` which will provide the best action $a$ (according to current knowledge, i.e., given observation sequence $\\mathbf{x}_{1:T}$) and associated uncertainty.\n",
    "\n",
    "**Beware** of the potential confusion here: $s_T$ represents the state of the environment but decision making (choosing action $a$) is based on observation $o = \\mathbf{x}_{1:T}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36d8ac0",
   "metadata": {
    "cell_marker": "r\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "The Environment is fully specified for you, as a Markov process \n",
    "$$\n",
    "    s_{t+1} \\sim p(\\cdot | s_t)\n",
    "$$\n",
    "with observations \n",
    "$$\n",
    "    \\mathbf{x}_t \\sim p( \\cdot | s_t)\n",
    "$$\n",
    "You can find this functionality in the `step` function. \n",
    "\n",
    "You should be able to see by looking at the code, that the rat moves by exactly $1$ square tile, either horizontally or vertically (i.e., taxicab-distance) per time step $t$, within the bounds of the of the room, starting at one of the entry points (uniformly at random). Insofar as the observation function: a 'crinkle' indicator is generated with probability $\\theta_1$ when over certain tiles (green, or orange), and with probability $0$ over other tiles; furthermore, it will invoke a 'rustling' is indicated with probability $\\theta_2$ over certain tiles (red, or orange), and $0$ otherwise. On orange tiles, both noises are caused independently of each other.\n",
    "\n",
    "\n",
    "#### Task 1: Generating trajectories \n",
    "\n",
    "Complete the `gen_traj` function to generate a trajectory (of length $T$). You have full acces to the environment, e.g., you can call `env.step(...)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28935fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_traj(env, T=5):\n",
    "    ''' Generate a path with associated observations.\n",
    "\n",
    "\n",
    "        Paramaters\n",
    "        ----------\n",
    "\n",
    "        T : int\n",
    "            how long is the path\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        o : (T,d)-shape array\n",
    "            sequence of observations\n",
    "        s : T-length array of states\n",
    "            sequence of tiles\n",
    "    '''\n",
    "    # TODO \n",
    "        # 1. init the return arrays\n",
    "    s = np.zeros(T, dtype=int)              # state sequence\n",
    "    o = np.zeros((T, env.d_obs), dtype=int) #observe sequence\n",
    "\n",
    "    # 2. init the environment usig None as the initial state\n",
    "    env._s= None\n",
    "    s[0], o[0] = env.step(None)\n",
    "    # 3. loop over the T timesteps\n",
    "    for t in range(1, T):\n",
    "        # 4. at each timestep, sample an action and observe the next state and observation\n",
    "        s[t] ,o[t]=  env.step(s[t-1]) \n",
    "    \n",
    "    return o, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf53ce7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Generate and view (plot) the trajectory\n",
    "ooo, sss = gen_traj(env,5)\n",
    "fig, ax = env.render(sss, ooo)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771627d8",
   "metadata": {
    "cell_marker": "r\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "It is important to realise that although we can have full access to the environment, as well as observations, we do not expect (in the 'real world') to see the true path $s_1,\\ldots,s_T$ and hence the challenge in estimating $s_T$. For this we will create an `Agent` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bce26ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent: \n",
    "\n",
    "    def __init__(self, env): \n",
    "        '''\n",
    "            env : Environment \n",
    "                of the type provided to you\n",
    "        '''\n",
    "        self.env = env\n",
    "\n",
    "    # TODO (optional): Add any auxilliary functions you might use here \n",
    "\n",
    "\n",
    "    def P_traj(self, ooo, M=-1):\n",
    "        '''\n",
    "        Provides full conditional distribution P(SSS | ooo) where SSS and ooo are sequences of length T.\n",
    "        $$\n",
    "            P( Y_1,\\\\ldots,Y_T | o_1,\\\\ldots,o_T )\n",
    "        $$\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        ooo : array_like(t, d)\n",
    "            t observations (of d dimensions each)\n",
    "\n",
    "        M : int\n",
    "            -1 indicates to use a brute force solution (exact recovery of the distribution) \n",
    "            M > 0 indicates to use M Monte Carlo simulations (this parameter is used in Week 2)\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        p : dict(str:float)\n",
    "            such that p[sss] = P(sss | ooo)\n",
    "            and if sss not in p, it implies P(sss | ooo) = 0\n",
    "\n",
    "            important: let sss be a string representation of the state sequence, separated by spaces, e.g., \n",
    "            the string representation of np.array([1,2,3,4],dtype=int) should be '1 2 3 4'. \n",
    "        '''        \n",
    "        p = {}\n",
    "        # TODO \n",
    "        T = len(ooo)  # 观测序列长度\n",
    "        # partial[t] 将存储所有长度为 t 的可能状态序列的 “联合概率” p(s_1,...,s_t, o_1,...,o_t)\n",
    "        partial = [dict() for _ in range(T+1)]\n",
    "        # 初始空路径概率 = 1\n",
    "        partial[0][()] = 1.0\n",
    "\n",
    "        # 逐步扩展到长度 T\n",
    "        for t in range(T):\n",
    "            # 当前要纳入第 t+1 个观测 ooo[t]\n",
    "            obs_t = ooo[t]  # 例如 [0,1] 或 [1,0] 等\n",
    "            for path_so_far, p_val in partial[t].items():\n",
    "                if t == 0:\n",
    "                    # 这是要采样 s_1\n",
    "                    for s1 in range(self.env.n_states):\n",
    "                        # p(s1) = env.P_1[s1]\n",
    "                        p_s1 = self.env.P_1[s1]\n",
    "                        # p(o_1 | s1)\n",
    "                        p_o1 = 1.0\n",
    "                        for j in range(self.env.d_obs):\n",
    "                            p_o1 *= self.env.P_O[s1, j, int(obs_t[j])]\n",
    "                        p_new = p_val * p_s1 * p_o1\n",
    "                        if p_new > 0:\n",
    "                            partial[t+1][(s1,)] = partial[t+1].get((s1,), 0.0) + p_new\n",
    "                else:\n",
    "                    # 已有 path_so_far = (s_1, ..., s_t)\n",
    "                    s_prev = path_so_far[-1]\n",
    "                    for s_new in range(self.env.n_states):\n",
    "                        p_trans = self.env.P_S[s_prev, s_new]  # p(s_{t+1} | s_t)\n",
    "                        # p(o_{t+1} | s_{t+1})\n",
    "                        p_obs = 1.0\n",
    "                        for j in range(self.env.d_obs):\n",
    "                            p_obs *= self.env.P_O[s_new, j, int(obs_t[j])]\n",
    "                        p_new = p_val * p_trans * p_obs\n",
    "                        if p_new > 0:\n",
    "                            new_path = path_so_far + (s_new,)\n",
    "                            partial[t+1][new_path] = partial[t+1].get(new_path, 0.0) + p_new\n",
    "\n",
    "        # 此时 partial[T] 里存放了所有长为T的路径 (s_1, ..., s_T) 对应的 p( s_1,...,s_T, o_1,...,o_T ).\n",
    "        # 我们要得到条件概率 p(...|oo) = 上式除以对所有路径的总和。\n",
    "        denom = sum(partial[T].values())\n",
    "        p = {}\n",
    "        for path, val in partial[T].items():\n",
    "            path_str = ' '.join(str(x) for x in path)\n",
    "            p[path_str] = val / denom\n",
    "        return p\n",
    "\n",
    "\n",
    "        \n",
    "    def P_S(self, o, t=-1, M=-1): \n",
    "        '''\n",
    "        Provide P(s_t | o) given observations o from 1,...,T.  \n",
    "\n",
    "        $$\n",
    "            P(S_t | o_1,...,o_T ).\n",
    "        $$\n",
    "        \n",
    "        The probability (distribution) of the t-th state, given the observed evidence 'o'.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        o : array_like(t,d)\n",
    "            up to t observations (of d dimensions each)\n",
    "\n",
    "        t : int\n",
    "            the state being queried, e.g., 3, or -1 for final state (corresponding to o[-1])\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        P : array_like(float,ndim=1) \n",
    "            such that P[s] = P(S_t = s | o_1,...,o_t)\n",
    "        '''\n",
    "        # TODO \n",
    "        T = len(o)\n",
    "        # 如果 t = -1，就默认看最终时刻 t = T\n",
    "        if t == -1:\n",
    "            t = T\n",
    "\n",
    "        # 先计算完整轨迹的后验分布\n",
    "        p_traj = self.P_traj(o)  # dict: path_str -> prob\n",
    "\n",
    "        # 求和所有满足 s_t = s 的路径概率\n",
    "        P = np.zeros(self.env.n_states)\n",
    "        for path_str, prob_val in p_traj.items():\n",
    "            path = [int(x) for x in path_str.split()]  # 转成真正的序列\n",
    "            # path[t-1] 就是 s_t （因为Python下标从0开始）\n",
    "            s_t = path[t-1]\n",
    "            P[s_t] += prob_val\n",
    "        \n",
    "        return P\n",
    "\n",
    "    def Q(self, o): \n",
    "        '''\n",
    "            Provide Q(o,a) for all a i.e., the value for any given a under observation o. \n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "\n",
    "            o : array_like(int,ndim=2)\n",
    "                t observations (of 2 bits each)\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "\n",
    "            Q : array_like(float,ndim=n_actions)\n",
    "                such that Q[a] is the value (expected reward) of action a.\n",
    "\n",
    "        '''\n",
    "        Q = np.zeros(self.env.n_states)\n",
    "        # TODO \n",
    "        Q= self.P_S(o, -1)\n",
    "        return Q\n",
    "\n",
    "    def act(self, obs): \n",
    "        '''\n",
    "        Decide on the best action to take, under the provided observation. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        obs : array_like(int,ndim=2)\n",
    "            t observations (of 2 bits each)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        a : int\n",
    "            the chosen action a\n",
    "        '''\n",
    "\n",
    "        a = -1\n",
    "        # TODO \n",
    "        Q_values = self.Q(obs)\n",
    "        a = np.argmax(Q_values)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f5f2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's instantiate our agent\n",
    "agent = Agent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b9f8ca",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "The Agent is responsible for receiving observation $o = \\mathbf{x}_{1:T}$ and producing prediction $a$, i.e., it implements $a = \\pi(o)$, i.e., its policy or `act` function as it is called here below. But let's implement the functionality step by step. \n",
    "\n",
    "#### Task 2: Complete the `P_traj` function above, which specifies  \n",
    "$$\n",
    "    P(S_{1:T} | \\mathbf{x}_{1:T})\n",
    "$$\n",
    "(returns a distribution, one number associated to each *possible* trajectory $s_1,\\ldots,s_T$).\n",
    "\n",
    "\n",
    "Implementation hint: For this, and remaining tasks. It may be useful to implement for the simple case of a single state observation (single time step) first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f207ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call your function\n",
    "P_joint = agent.P_traj(ooo)\n",
    "\n",
    "# Check should sum to 1 (or close enough)\n",
    "probs = list(P_joint.values())\n",
    "assert abs(sum(probs) - 1) <= 0.05\n",
    "\n",
    "# Extract possible paths\n",
    "paths = [np.fromstring(k, sep=' ') for k in P_joint.keys()] \n",
    "\n",
    "# Take some samples\n",
    "sample_indices = np.random.choice(len(probs), size=10, p=probs)\n",
    "trajs = [paths[i].astype(int) for i in sample_indices]\n",
    "\n",
    "# \n",
    "\n",
    "fig, ax = env.render(sss, ooo, paths=trajs, title=r\"$s_1,\\ldots,s_T \\sim P(\\cdot \\mid o_1,\\ldots,o_T)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6c4baf",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "#### Task 3: complete the `P_S(o,t)` function, which implements \n",
    "$$\n",
    "    P(S_{t} | \\mathbf{x}_{1:T})\n",
    "$$\n",
    "(returns a distribution, one number for each possible $s_t$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a874ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise P(S_t)\n",
    "s = sss#[-1]\n",
    "o = ooo#[-1]\n",
    "\n",
    "P_S = agent.P_S(o)\n",
    "\n",
    "fig, ax = env.render(sss, ooo, dgrid=P_S, title=r\"$P(S | \\vec{x}_1,\\ldots,\\vec{x}_T)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec93c366",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "#### Task 4: complete the value function `Q`, which implements \n",
    "$$\n",
    "    Q(S, \\mathbf{x}_{1:T}) = V(S)\n",
    "$$\n",
    "(i.e., one number for each state). This is based on $P(S_t)$ but takes into account the reward. \n",
    "\n",
    "In the following you can visualise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27d7377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the value function\n",
    "Q_A = agent.Q(o)\n",
    "fig, ax = env.render(sss, ooo, dgrid=Q_A, title=r\"$V(S | \\vec{x}_1,\\ldots,\\vec{x}_T)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41241ba7",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "#### Task: Taking an Action\n",
    "\n",
    "Complete the `act` function, which implements \n",
    "$$\n",
    "    a = \\pi(s)\n",
    "$$\n",
    "This should be straightforward from the previous; acting to maximize value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524e18eb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "s_T = sss[-1]\n",
    "a = agent.act(ooo)\n",
    "r = env.rwd(s_T,a)\n",
    "fig, ax = env.render(sss, ooo, a_star=a, title=\"$r(%d,%d) = %2.1f$\" % (s_T,a,r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009c91cd",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "#### Recap \n",
    "\n",
    "Your agent has observed evidence $\\mathbf{x}_1,\\ldots,\\mathbf{x}_T$, and queried the model, according to your beliefs (environment dynamics). Time to make a decision. Which action to take? The answer: \n",
    "$$\n",
    "    a_* = \\text{argmax}_a \\mathbb{E}_{S_T \\sim P(S_T | \\mathbf{x}_{1:T})}[ r(S_T, a) ]\n",
    "$$\n",
    "\n",
    "Note your uncertainty about the final state $S_T$. \n",
    "\n",
    "In this scenario the action does not affect future observations (because $s_T$ is the final observation), thus you are essentially making an estimate:\n",
    "$$\n",
    "    a = \\hat s_{T} = \\pi(\\mathbf{x}_{1:T})\n",
    "$$\n",
    "\n",
    "We have referred to 'value' and `Q` inline with reinforcement learning terminology. \n",
    "\n",
    "Did you get the maximum reward? Remember, an agent is not expected to obtain maximum reward, it is expected to obtain (close to) maximum *expected* reward, under the uncertainty implied by the environment. The following code block will help ascertain the 'success rate' of your agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9ff869",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "#### Evaluating the Agent\n",
    "\n",
    "If you have implemented all the above tasks, then we should be able to evaluate the performance of your agent over a number of simulations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8f210b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Check average performace over n trials\n",
    "n = 100\n",
    "r_avg = 0\n",
    "for i in range(n): \n",
    "    ooo, sss = gen_traj(env,5)\n",
    "    a = agent.act(ooo)\n",
    "    r_avg += env.rwd(sss[-1],a) / n\n",
    "print(r_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a245e3",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Conclusion (so what?)\n",
    "\n",
    "This was just a toy example, but consider the fundamental concepts here (we will be using them again); we designed an intelligent decision-making agent involving *perception* (observation), *knowledge* (representation), *reasoning* (inference), and *acting* (decision-making). There are two limitations here: the toy nature of the environment (maybe your solution does not scale up to large $d$ or large $T$) and the requirement for a hand-coded environment. Next week we will look at efficient inference, and learning (including, representation learning) which allows to overcome these limitations; toward not only an autonomous agent, but a capable and scalable autonomous agent. "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
