{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team 6 Lunar Lander Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT Imports\n",
    "\n",
    "import math\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List, Optional, Tuple, Union, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Lunar Lander problem with Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of focusing on the algorithms, we will use standard environments provided\n",
    "by the Gymnasium framework.\n",
    "As a reminder, this environment is described [here](https://gymnasium.org.cn/environments/box2d/lunar_lander/).\n",
    "\n",
    "The action indices are outlined below:\n",
    "\n",
    "| Action Index | Action     |\n",
    "|--------------|------------|\n",
    "| 0            | nothing  |\n",
    "| 1            |  left orientation engine  |\n",
    "| 2            |  main engine |\n",
    "| 3            |  right orientation engine |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_labels = {0: \"Nothing\", 1: \"Left Engine\", 2: \"Main Engine\", 3: \" Right Engine\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_EVERY_N_EPISODES = 50\n",
    "\n",
    "def q_learning(\n",
    "    environment: gym.Env,\n",
    "    alpha: float = 0.1,\n",
    "    alpha_factor: float = 0.9995,\n",
    "    gamma: float = 0.99,\n",
    "    epsilon: float = 0.5,\n",
    "    num_episodes: int = 10000,\n",
    "    display: bool = False,\n",
    ") -> Tuple[np.ndarray, List[np.ndarray], List[float]]:\n",
    "    \"\"\"\n",
    "    Perform Q-learning on a given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    environment : gym.Env\n",
    "        The environment to learn in.\n",
    "    alpha : float, optional\n",
    "        The learning rate, between 0 and 1. By default 0.1\n",
    "    alpha_factor : float, optional\n",
    "        The factor to decrease alpha by each episode, by default 0.9995\n",
    "    gamma : float, optional\n",
    "        The discount factor, between 0 and 1. By default 0.99\n",
    "    epsilon : float, optional\n",
    "        The probability of choosing a random action, by default 0.5\n",
    "    num_episodes : int, optional\n",
    "        The number of episodes to run, by default 10000\n",
    "    display : bool, optional\n",
    "        Whether to display the Q-table (every DISPLAY_EVERY_N_EPISODES episodes), by default False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The learned Q-table.\n",
    "        Each row corresponds to a state, and each column corresponds to an action.\n",
    "        In the frozen lake environment, there are 16 states and 4 actions thus the Q-table has a shape of (16, 4).\n",
    "        For instance, q_array[0, 3] is the Q-value (estimation of the expected reward) for performing action 3 (\"move up\") in state 0 (the top left square).\n",
    "    \"\"\"\n",
    "    # Initialize the history of the Q-table and learning rate\n",
    "    q_array_history = []\n",
    "    alpha_history = []\n",
    "\n",
    "    observation_space = cast(gym.spaces.Discrete, environment.observation_space)\n",
    "    action_space = cast(gym.spaces.Discrete, environment.action_space)\n",
    "\n",
    "    # Get the number of states in the environment\n",
    "    num_states = observation_space.n\n",
    "\n",
    "    # Get the number of actions in the environment\n",
    "    num_actions = action_space.n\n",
    "\n",
    "    # Initialize the Q-table to zeros\n",
    "    q_array = np.zeros([num_states, num_actions])\n",
    "\n",
    "    # Loop over the episodes\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        # Display the Q-table every DISPLAY_EVERY_N_EPISODES episodes if display is True\n",
    "        if display and episode_index % DISPLAY_EVERY_N_EPISODES == 0:\n",
    "            display_qtable(q_array, title=\"Q table\")\n",
    "\n",
    "        # Save the current Q-table and learning rate\n",
    "        q_array_history.append(q_array.copy())\n",
    "        alpha_history.append(alpha)\n",
    "\n",
    "        # Decrease the learning rate if alpha_factor is not None\n",
    "        if alpha_factor is not None:\n",
    "            alpha = alpha * alpha_factor\n",
    "\n",
    "        # TODO...\n",
    "        # 1. Reset the environment (new Gym typically returns (obs, info))\n",
    "        obs, info = environment.reset()\n",
    "        state = obs\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Epsilon-greedy action selection\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.randint(num_actions)  # random action\n",
    "            else:\n",
    "                action = np.argmax(q_array[state])       # greedy action\n",
    "\n",
    "            # Execute the action in the environment\n",
    "            next_obs, reward, terminated, truncated, info = environment.step(action)\n",
    "            next_state = next_obs\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Q-learning update:\n",
    "            # Q(S, A) ← Q(S, A) + α * [ R + γ * max_{a'} Q(S', a') - Q(S, A) ]\n",
    "            best_next_action = np.argmax(q_array[next_state])  # best action in next state\n",
    "            td_target = reward + gamma * q_array[next_state, best_next_action] * (1 - done)\n",
    "            q_array[state, action] += alpha * (td_target - q_array[state, action])\n",
    "\n",
    "            # Transition to the next state\n",
    "            state = next_state        \n",
    "\n",
    "    # Return the learned Q-table\n",
    "    return q_array, q_array_history, alpha_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4187bf57064f888680940eff238211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the environment and set the maximum number of steps per episode\n",
    "environment = gym.make(\"FrozenLake-v1\", max_episode_steps=1000)\n",
    "\n",
    "# Apply Q-learning to calculate the Q-table for the FrozenLake environment\n",
    "q_array_ex3, q_array_history_ex3, alpha_history_ex3 = q_learning(environment, display=False)\n",
    "\n",
    "environment.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
